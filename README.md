# Spot-Aware Scaler: Zero-Downtime Spot Instance Protection ğŸš€

[![Kubernetes](https://img.shields.io/badge/kubernetes-%23326ce5.svg?style=for-the-badge&logo=kubernetes&logoColor=white)](https://kubernetes.io/)
[![AWS](https://img.shields.io/badge/AWS-%23FF9900.svg?style=for-the-badge&logo=amazon-aws&logoColor=white)](https://aws.amazon.com/)
[![Go](https://img.shields.io/badge/shell-%23000000.svg?style=for-the-badge&logo=gnu-bash&logoColor=white)](https://www.gnu.org/software/bash/)

A Kubernetes controller that automatically protects single-replica workloads from AWS spot instance interruptions by implementing **proactive scaling** to achieve **zero-downtime** during node terminations.

## ğŸ¯ Problem Solved

**The Challenge:**
- Single-replica deployments on spot instances experience downtime during spot interruptions
- Race condition: old pod terminates before new pod becomes ready
- Critical for QA environments where brief outages disrupt testing workflows

**The Solution:**
- **Proactive scaling**: Scale 1â†’2 replicas when spot interruption detected
- **Zero downtime**: New pod ready before old pod terminates  
- **Automatic cleanup**: Scale back to 1 replica after 5 minutes
- **Restart resilient**: Uses persistent storage instead of background processes

## âœ¨ Key Features

- âœ… **Zero Downtime Protection** - Eliminates race conditions during spot interruptions
- âœ… **One-Line Setup** - Just add `spot-aware: enabled` label to deployments
- âœ… **Namespace Targeting** - Whitelist/blacklist specific namespaces
- âœ… **Cost Effective** - Only scales during interruptions (~5 minutes)
- âœ… **Restart Resilient** - Survives controller restarts using persistent ConfigMaps
- âœ… **Production Ready** - Handles multiple simultaneous interruptions
- âœ… **No Code Changes** - Works with existing applications

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AWS Spot      â”‚â”€â”€â”€â–¶â”‚ Node Termination â”‚â”€â”€â”€â–¶â”‚  Spot-Aware     â”‚
â”‚   Interruption  â”‚    â”‚    Handler       â”‚    â”‚    Scaler       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ Cordon Node      â”‚    â”‚ Proactive Scale â”‚
                       â”‚ (Scheduling      â”‚    â”‚ 1 â†’ 2 Replicas  â”‚
                       â”‚  Disabled)       â”‚    â”‚                 â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                         â–¼
                                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                               â”‚ Persistent      â”‚
                                               â”‚ Scale-Down      â”‚
                                               â”‚ Schedule        â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1. Deploy the System

```bash
# Deploy configuration
kubectl apply -f k8s-manifests/spot-scaler-config.yaml

# Deploy the controller
kubectl apply -f k8s-manifests/spot-aware-scaler.yaml

# Deploy node termination handler (if not already present)
kubectl apply -f k8s-manifests/node-safety-controller.yaml

# Deploy spot node pool (if using Karpenter)
kubectl apply -f k8s-manifests/karpenter-nodepool.yaml
```

### 2. Protect Your Workloads

Add the label to any deployment you want to protect:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: your-application
  labels:
    spot-aware: enabled  # ğŸ‘ˆ Only change needed!
spec:
  replicas: 1
  # ... rest of your deployment
```

### 3. Configure Namespace Targeting (Optional)

```yaml
# Edit the ConfigMap to target specific namespaces
apiVersion: v1
kind: ConfigMap
metadata:
  name: spot-aware-scaler-config
data:
  target-namespaces: "your-qa-ns,your-dev-ns"  # Whitelist
  namespace-mode: "whitelist"
```

## ğŸ“Š How It Works - Step by Step

### Real-World Flow Example

1. **Normal Operation**
   ```
   Deployment: 1 replica running on spot node
   Status: Service available âœ…
   ```

2. **Spot Interruption Detected** 
   ```
   AWS: Sends 2-minute termination warning
   NTH: Cordons the spot node (SchedulingDisabled)
   Scaler: Detects cordoned node with protected workload
   ```

3. **Proactive Scaling** 
   ```
   Action: Immediately scale deployment 1â†’2 replicas
   Result: New pod scheduled on safe node
   Time: < 1 second response time
   ```

4. **Zero-Downtime Transition**
   ```
   Pod 1: Still running on spot node (serving traffic)
   Pod 2: Starting on safe node
   Load Balancer: Routes to both pods when ready
   ```

5. **Node Termination**
   ```
   AWS: Terminates spot node
   Pod 1: Gracefully terminated
   Pod 2: Continues serving traffic
   Downtime: 0 seconds âœ…
   ```

6. **Automatic Cleanup**
   ```
   Wait: 5 minutes (configurable)
   Action: Scale back to 1 replica
   Result: Cost-optimized single replica
   ```

## ğŸ”§ Configuration

### Core Settings

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spot-aware-scaler-config
data:
  # Namespace targeting
  target-namespaces: "namespace1,namespace2"
  namespace-mode: "whitelist"  # or "blacklist" or "all"
  
  # Pod selection
  protected-labels: "spot-aware=enabled"
  
  # Timing configuration
  scale-up-delay: "10"      # Delay before scaling up (seconds)
  scale-down-delay: "300"   # Wait before scaling down (seconds)
  check-interval: "30"      # Monitoring frequency (seconds)
```

### Namespace Modes

| Mode | Description | Example |
|------|-------------|---------|
| `whitelist` | Only monitor specified namespaces | `"qa-team1,qa-team2"` |
| `blacklist` | Monitor all except specified | `"production,staging"` |
| `all` | Monitor all namespaces | `""` |

## ğŸ“‹ Required Files

The system requires these 4 core files:

```
k8s-manifests/
â”œâ”€â”€ spot-aware-scaler.yaml      # Main controller
â”œâ”€â”€ spot-scaler-config.yaml     # Configuration
â”œâ”€â”€ node-safety-controller.yaml # Node Termination Handler
â””â”€â”€ karpenter-nodepool.yaml     # Spot node pool (optional)
```

## ğŸ” Monitoring & Troubleshooting

### Check System Status

```bash
# Verify scaler is running
kubectl get deployment spot-aware-scaler -n spot-scaling

# Check configuration
kubectl get configmap spot-aware-scaler-config -o yaml

# Monitor scaler logs
kubectl logs -f deployment/spot-aware-scaler -n spot-scaling
```

### Watch for Spot Interruptions

```bash
# Check for cordoned nodes
kubectl get nodes | grep SchedulingDisabled

# Monitor protected deployments
kubectl get deployments -l spot-aware=enabled --all-namespaces

# Watch scaling events
kubectl get events --all-namespaces --field-selector reason=ScalingReplicaSet
```

### Debug Scale-Down Issues

```bash
# Check for pending scale-down schedules
kubectl get configmaps -n spot-scaling | grep scale-down

# View schedule details
kubectl get configmap scale-down-DEPLOYMENT-NAME -n spot-scaling -o yaml
```

## ğŸ¯ Use Cases

### Perfect For:
- **QA/Development environments** with single-replica services
- **Cost-sensitive workloads** on spot instances
- **Stateless applications** that can't afford downtime
- **CI/CD pipelines** running on spot infrastructure

### Not Suitable For:
- **Stateful services** with complex data migration needs
- **Applications** that already have multiple replicas
- **Production workloads** requiring more sophisticated HA strategies

## ğŸ”’ Security & Permissions

The controller requires these permissions:

- **Cluster-wide**: Read nodes, deployments, pods
- **Namespace-scoped**: Manage ConfigMaps in controller namespace
- **RBAC**: Least-privilege access with service account

## ğŸš¦ Advanced Features

### Restart Resilience
- Uses Kubernetes ConfigMaps instead of background processes
- Survives controller pod restarts and updates
- Automatically resumes interrupted scale-down operations

### Multi-Interruption Handling
- Handles multiple simultaneous spot interruptions
- Each deployment gets independent scale-down schedule
- Prevents interference between different workloads

### Intelligent Detection
- Supports multiple deployment selector patterns (`app`, `component`, `name`)
- Compatible with various deployment configurations
- Graceful handling of edge cases

## ğŸ“ˆ Benefits

| Metric | Before | After |
|--------|--------|-------|
| **Downtime** | 30-60 seconds | 0 seconds |
| **Setup Complexity** | Complex HA setup | Single label |
| **Cost Impact** | Always 2+ replicas | 5 min temporary scaling |
| **Maintenance** | Manual intervention | Fully automated |

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test with real spot interruptions
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- AWS Node Termination Handler team for the foundation
- Kubernetes community for the scheduling intelligence
- Karpenter team for efficient spot instance management

---

**Ready to eliminate spot interruption downtime?** Add `spot-aware: enabled` to your deployments and experience zero-downtime spot instance protection! ğŸ‰
